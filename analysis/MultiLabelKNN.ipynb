{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd() / '..' / 'data'\n",
    "targets_file = data_dir / 'labels.csv'\n",
    "similarity_matrix_file = data_dir / 'processed' / 'similarity.npy'\n",
    "filtered_texts_listing_file = data_dir / 'filtered_texts_listing.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_iterative_stratification(dataset, subset_ratios):\n",
    "    \"\"\"Split multi-label dataset into disjoint subsets based on even distribution of labels\"\"\"\n",
    "    subset_ratios = np.asarray(subset_ratios)\n",
    "    k = len(subset_ratios)\n",
    "\n",
    "    subset_counts = (subset_ratios * len(dataset)).round(decimals=2)\n",
    "\n",
    "    labels = pd.DataFrame(data={'total':dataset.sum()}).sort_values('total')\n",
    "    labels = labels.assign(count_index = range(0, len(labels)))\n",
    "\n",
    "    label_subset_counts = np.matrix(labels.total.values).transpose() \\\n",
    "                          * np.matrix(subset_ratios)\n",
    "    label_subset_counts = np.around(label_subset_counts, decimals=2)\n",
    "    \n",
    "    groups = pd.DataFrame(data={'group': -1}, index=dataset.index)\n",
    "    remaining_documents = dataset.loc[groups.group == -1,:]\n",
    "    while len(remaining_documents) > 0:\n",
    "        label_counts = remaining_documents.sum()\n",
    "        min_label = label_counts[label_counts > 0].idxmin()\n",
    "\n",
    "        applicable_documents = remaining_documents.loc[\n",
    "            remaining_documents.loc[:, min_label] > 0, :]\n",
    "        for document_id, row_data in applicable_documents.iterrows():\n",
    "            max_size_label = label_subset_counts[\n",
    "                labels.count_index[min_label], ].max()\n",
    "            max_subset_by_label = np.nonzero(\n",
    "                label_subset_counts[labels.count_index[min_label], ] == max_size_label)[0]\n",
    "\n",
    "            if len(max_subset_by_label) == 1:\n",
    "                max_subset = max_subset_by_label[0]\n",
    "            else:\n",
    "                max_size_count = subset_counts[max_subset_by_label].max()\n",
    "                max_subset_by_count = np.nonzero(\n",
    "                    subset_counts == max_size_count)[0]\n",
    "\n",
    "                if len(max_subset_by_count) == 1:\n",
    "                    max_subset = max_subset_by_count[0]\n",
    "                else:\n",
    "                    max_subset = np.random.choice(max_subset_by_count, 1)[0]\n",
    "\n",
    "            groups.loc[document_id, 'group'] = max_subset\n",
    "            label_subset_counts[\n",
    "                remaining_documents.loc[document_id, :].nonzero()[0],\n",
    "                max_subset] -= 1\n",
    "            subset_counts[max_subset] -= 1\n",
    "\n",
    "        remaining_documents = dataset.loc[groups.group == -1,:]\n",
    "        \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior(training_matrix, smoothing=1):\n",
    "    m = len(training_matrix.coords['document_id'])\n",
    "\n",
    "    prior_has_label = ((smoothing + training_matrix.sum('document_id')) / (2 * smoothing + m))\n",
    "    prior_has_not_label = 1 - prior_has_label\n",
    "    \n",
    "    return (prior_has_label, prior_has_not_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest_neighbors(instance, similarity_matrix, k):\n",
    "    topk_idx = np.argpartition(\n",
    "        similarity_matrix.sel(document_id_a=instance).values,\n",
    "        -(k + 1))[-(k + 1):]\n",
    "\n",
    "    not_instance = np.nonzero(\n",
    "            (similarity_matrix.coords['document_id_b'][topk_idx] != instance).values)[0]\n",
    "    topk_idx = topk_idx[not_instance]\n",
    "    topk_idx = similarity_matrix.coords['document_id_b'][topk_idx]\n",
    "\n",
    "    return topk_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posterior(training_matrix, training_similarity, k, smoothing = 1):\n",
    "    counting_membership_ids = []\n",
    "    counting_membership_vectors = []\n",
    "    for document_id in training_matrix.coords['document_id'].values:\n",
    "        # for each training document, get k nearest neighbors\n",
    "        topk_neighbors = get_k_nearest_neighbors(document_id, training_similarity, k)\n",
    "        counting_membership_ids.append(document_id)\n",
    "        counting_vector = training_matrix.sel(document_id=topk_neighbors.values).sum('document_id')\n",
    "        counting_membership_vectors.append(counting_vector)\n",
    "\n",
    "    counting_membership = xr.concat(\n",
    "        counting_membership_vectors,\n",
    "        pd.Index(\n",
    "            counting_membership_ids,\n",
    "            name='document_id'))\n",
    "\n",
    "    has_label_counts = xr.DataArray(\n",
    "        data=np.zeros((len(training_matrix.coords['label']), k + 1),\n",
    "                      dtype='int64'),\n",
    "        coords=[training_matrix.coords['label'].values, range(k + 1)],\n",
    "        dims=['label', 'k'])\n",
    "\n",
    "    has_not_label_counts = xr.DataArray(\n",
    "        data=np.zeros((len(training_matrix.coords['label']), k + 1),\n",
    "                      dtype='int64'),\n",
    "        coords=[training_matrix.coords['label'].values, range(k + 1)],\n",
    "        dims=['label', 'k'])\n",
    "\n",
    "    for label in training_matrix.coords['label'].values:\n",
    "        has_bin_counts = np.bincount(\n",
    "            counting_membership.sel(label=label)[\n",
    "                training_matrix.sel(label=label) == 1])\n",
    "\n",
    "        padded_has_counts = np.pad(\n",
    "            has_bin_counts,\n",
    "            pad_width=(0, (k + 1) - len(has_bin_counts)),\n",
    "            mode='constant',\n",
    "            constant_values=(0, 0))\n",
    "\n",
    "        has_label_counts.sel(label=label).values += padded_has_counts\n",
    "\n",
    "        not_bin_counts = np.bincount(\n",
    "            counting_membership.sel(label=label)[\n",
    "                training_matrix.sel(label=label) == 0])\n",
    "\n",
    "        padded_not_counts = np.pad(\n",
    "            not_bin_counts,\n",
    "            pad_width=(0, (k + 1) - len(not_bin_counts)),\n",
    "            mode='constant',\n",
    "            constant_values=(0, 0))\n",
    "\n",
    "        has_not_label_counts.sel(label=label).values += padded_not_counts\n",
    "\n",
    "    # posterior probability that target has # of neighbors with given label\n",
    "    # conditional that it also has that label\n",
    "    post_cond_has_label = (smoothing + has_label_counts) / (smoothing * (k + 1)\n",
    "                                                           + has_label_counts.sum(dim='k'))\n",
    "\n",
    "    # posterior probability that target has # of neighbors with given label\n",
    "    # conditional that it does not have that label\n",
    "    post_cond_not_label = (smoothing + has_not_label_counts) / (smoothing * (k + 1)\n",
    "                                                                + has_not_label_counts.sum(dim='k'))\n",
    "    \n",
    "    return (post_cond_has_label, post_cond_not_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_data(testing_matrix, priors, posteriors, testing_similarity, k):\n",
    "    counting_membership_ids = []\n",
    "    counting_membership_vectors = []\n",
    "    for document_id in testing_matrix.coords['document_id'].values:\n",
    "        # for each training document, get k nearest neighbors\n",
    "        topk_neighbors = get_k_nearest_neighbors(document_id, testing_similarity, k)\n",
    "        counting_membership_ids.append(document_id)\n",
    "        counting_vector = testing_matrix.sel(document_id=topk_neighbors.values).sum('document_id')\n",
    "        counting_membership_vectors.append(counting_vector)\n",
    "\n",
    "    counting_membership = xr.concat(\n",
    "        counting_membership_vectors,\n",
    "        pd.Index(\n",
    "            counting_membership_ids,\n",
    "            name='document_id'))\n",
    "\n",
    "    predictions = (priors[0] * posteriors[0].sel(k=counting_membership) \n",
    "                   > \n",
    "                   priors[1] * posteriors[1].sel(k=counting_membership)).astype('int64')\n",
    "\n",
    "    predicted_ranking = (priors[0] * posteriors[0].sel(k=counting_membership)) /  (\n",
    "        priors[0] * posteriors[0].sel(k=counting_membership)\n",
    "        + priors[1] * posteriors[1].sel(k=counting_membership))\n",
    "    \n",
    "    return predictions, predicted_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(expected, predicted):\n",
    "    matches = (expected != predicted)\n",
    "    hamming_loss = matches.sum() / \\\n",
    "                   (len(matches.coords['document_id']) *\n",
    "                    len(matches.coords['label']))\n",
    "    return hamming_loss.values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_error(testing_matrix, predictions, label_probs):\n",
    "    max_label_idx = label_probs.argmax(dim='label')\n",
    "    matches = predictions.isel(label=max_label_idx) \\\n",
    "              != testing_matrix.isel(label=max_label_idx)\n",
    "    one_error = (matches.sum() / len(matches.coords['document_id']))\n",
    "    return one_error.values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(testing_matrix, ranking):\n",
    "    total = np.where((testing_matrix == 1).transpose().values, ranking.values, 0).max(axis=0).sum()\n",
    "    total -= len(testing_matrix.coords['document_id'])\n",
    "    coverage = total / len(testing_matrix.coords['document_id'])\n",
    "    \n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subset_data(groups, subsets, dataset):\n",
    "    subset_data = {}\n",
    "    for subset in subsets:\n",
    "        dataset_subset = dataset.loc[groups.group == subset]\n",
    "        dataset_matrix_subset = xr.DataArray(dataset_subset.as_matrix(),\n",
    "                      coords=[dataset_subset.index.values, dataset_subset.columns.values],\n",
    "                      dims=['document_id', 'label'])\n",
    "\n",
    "        subset_data[subset] = dataset_matrix_subset\n",
    "        \n",
    "    return subset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_selected_subsets(selected_subsets, subset_data, similarity_matrix):\n",
    "    dataset_subsets = (subset_data[subset]\n",
    "                       for subset in selected_subsets)\n",
    "    dataset_matrix = xr.concat(dataset_subsets, dim='document_id')\n",
    "    \n",
    "    similarity_matrix_subset = similarity_matrix.sel(\n",
    "        document_id_a=dataset_matrix.document_id.values,\n",
    "        document_id_b=dataset_matrix.document_id.values)\n",
    "    \n",
    "    return dataset_matrix, similarity_matrix_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_labels(instance, data_matrix):\n",
    "    return list(data_matrix.coords['label'][data_matrix.sel(document_id=instance) == 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_listing = pd.read_csv(filtered_texts_listing_file, names=['document_id'])\n",
    "filtered_listing = filtered_listing.document_id.str.split('.').str.get(0).astype('int64')\n",
    "\n",
    "dataset = pd.read_csv(targets_file, header=None, names=['id', 'subject'])\n",
    "dataset.subject = dataset.subject.astype('category')\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='').groupby('id').max()\n",
    "dataset = dataset.loc[filtered_listing.values, :]\n",
    "\n",
    "k = 10\n",
    "subset_ratios = np.repeat(0.1, k)\n",
    "\n",
    "similarity_matrix_raw = np.memmap(\n",
    "    similarity_matrix_file,\n",
    "    dtype='float32', mode='r',\n",
    "    shape=(dataset.shape[0], dataset.shape[0]))\n",
    "\n",
    "document_ids = filtered_listing.sort_values().values\n",
    "\n",
    "similarity_matrix = xr.DataArray(\n",
    "    data=similarity_matrix_raw,\n",
    "    coords=[document_ids, document_ids],\n",
    "    dims=['document_id_a', 'document_id_b'])\n",
    "\n",
    "groups = multilabel_iterative_stratification(dataset, subset_ratios)\n",
    "subsets = np.array(groups.group.unique())\n",
    "subset_data = split_subset_data(groups, subsets, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_hamming_loss = []\n",
    "rounds_one_error = []\n",
    "rounds_coverage = []\n",
    "\n",
    "for round_id, test_subsets in enumerate(combinations(subsets, 1)):\n",
    "    train_subsets = subsets[np.bitwise_not(np.isin(subsets, test_subsets))]\n",
    "    \n",
    "    training_matrix, training_similarity = join_selected_subsets(train_subsets, subset_data,\n",
    "                                                                similarity_matrix)\n",
    "    testing_matrix, testing_similarity = join_selected_subsets(test_subsets, subset_data,\n",
    "                                                              similarity_matrix)\n",
    "\n",
    "    priors = calculate_prior(training_matrix, smoothing=1)\n",
    "    posteriors = calculate_posterior(training_matrix, training_similarity, smoothing=1, k=k)\n",
    "\n",
    "    predictions, label_probs = evaluate_test_data(\n",
    "        testing_matrix, priors, posteriors, testing_similarity, k)\n",
    "\n",
    "    ordering = label_probs.argsort(axis=0).sel(label=slice(None, None, -1)) \n",
    "    ranking = ordering.argsort(axis=0) + 1\n",
    "    \n",
    "    rounds_hamming_loss.append(hamming_loss(testing_matrix, predictions))\n",
    "    rounds_one_error.append(one_error(testing_matrix, predictions, label_probs))\n",
    "    rounds_coverage.append(coverage(testing_matrix, ranking))\n",
    "\n",
    "rounds_hamming_loss = np.array(rounds_hamming_loss)\n",
    "rounds_one_error = np.array(rounds_one_error)\n",
    "rounds_coverage = np.array(rounds_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02766217751184475, 0.3631220590574452, 11.947069346372976)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounds_hamming_loss.mean(), rounds_one_error.mean(), rounds_coverage.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
